# -*- coding: utf-8 -*-
"""House Price Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19cF2DEO9nc7tMsgbF15exzHtr85Hn3Cs

### Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import scipy
import scipy.stats as stats
from scipy.special import boxcox1p
import missingno as msno
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_squared_error, make_scorer
from sklearn.svm import SVR
from sklearn.linear_model import Lasso, LinearRegression
from sklearn.ensemble import RandomForestRegressor

from lightgbm import LGBMRegressor
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import mutual_info_regression, f_regression
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, KFold, RepeatedKFold
from sklearn.pipeline import Pipeline

from IPython.display import display
import warnings
warnings.filterwarnings('ignore')

"""### Read data"""

train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')
# sub = pd.read_csv('Data/sample_submission.csv')

print('Train Data: ')
display(train.head())
print('Test Data: ')
display(test.head())

print('Train shape: ',train.shape)
print('Test shape: ',test.shape)

"""### Explore Data"""

print('The number of columns in Train set : {}'.format(len(train.columns.tolist())))
print('The number of columns in Test set : {}'.format(len(test.columns.tolist())))

intVar = train.select_dtypes(include = ['int']).columns.tolist()
floatVar = train.select_dtypes(include = ['float']).columns.tolist()
objectVar = train.select_dtypes(include = ['object']).columns.tolist()



# drop_columns = ['Id']
drop_columns = ['Id', 'LowQualFinSF', 'MiscVal', 'BsmtHalfBath', 'BsmtFinSF2']
train.drop(columns=drop_columns, axis=1, inplace=True)
test.drop(columns=drop_columns, axis=1, inplace=True)

train.duplicated().sum()

ntrain = train.shape[0]
ntest = test.shape[0]
Target = train['SalePrice']
Data = pd.concat((train, test)).reset_index(drop=True)
Data.drop(['SalePrice'], axis=1, inplace=True)
print("Data size is : {}".format(Data.shape))

"""### Missing Data"""

miss_features = [col for col in Data.columns.tolist() if Data[col].isnull().any()]
print('The number of features that have missing values: {}'.format(len(miss_features)))
msno.bar(Data[miss_features])
# miss_features = [col for col in train.columns.tolist() if train[col].isnull().any()]
# print('The number of features that have missing values in Train: {}'.format(len(miss_features)))
# display(msno.bar(train[miss_features]))

na_data = (Data.isnull().sum() / len(Data)) * 100
na_data = na_data.drop(na_data[na_data == 0].index).sort_values(ascending=False)[:30]
missing_data = pd.DataFrame({'Missing Percentage' :na_data})
missing_data.head(20)
# na_data = (train.isnull().sum() / len(train)) * 100
# na_data = na_data.drop(na_data[na_data == 0].index).sort_values(ascending=False)[:30]
# missing_data = pd.DataFrame({'Missing Percentage' :na_data})
# missing_data.head(20)

Data['LotFrontage'].fillna(Data['LotFrontage'].median(), inplace=True)

print('The number of Null values in LotFrontage: ', Data['LotFrontage'].isnull().sum())

# 'LowQualFinSF', 'MiscVal', 'BsmtFinSF2'
for col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:
    Data[col] = Data[col].fillna('None')

for col in ['GarageYrBlt', 'GarageArea', 'GarageCars']:
    Data[col] = Data[col].fillna(0)

for col in ['BsmtFinSF1','BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath']:
    Data[col] = Data[col].fillna(0)

for col in ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']:
    Data[col] = Data[col].fillna('None')

Data['MasVnrType'] = Data['MasVnrType'].fillna('None')
Data['MasVnrArea'] = Data['MasVnrArea'].fillna(0)

Data['Functional'].fillna('Typ', inplace=True)

# data description says NA means typical
Data['Electrical'].fillna(Data['Electrical'].mode()[0], inplace=True)

print('The number of missing values: ', sum(Data.isnull().sum()))

print('Utilities value counts: \n', Data['Utilities'].value_counts())
Data.drop('Utilities', axis=1, inplace=True)

print('DType for YrSold is :', Data['YrSold'].dtype)
print('DType for MoSold is :', Data['MoSold'].dtype)
print('DType for YearBuilt is: ', Data['YearBuilt'].dtype)

Data['TotalSF'] = Data['TotalBsmtSF'] + Data['1stFlrSF'] + Data['2ndFlrSF']

"""### Encoder

#### Ordinal
"""

# 14 features
objVar = ['CentralAir', 'LandSlope', 'ExterQual', 'ExterCond',
             'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',
             'HeatingQC', 'KitchenQual', 'GarageQual', 'GarageCond']
# objVar = train.select_dtypes(include = ['object']).columns.tolist()
encoder = LabelEncoder()
for col in objVar:
    encoder.fit(list(Data[col].values))
    Data[col] = encoder.transform(list(Data[col].values))
print('Data Shape: ',Data.shape)

num_fe = ['ScreenPorch', 'EnclosedPorch', 'KitchenAbvGr', 'LandSlope', '3SsnPorch', 'LotArea', 'PoolArea']
skewed_feats = Data[num_fe].apply(lambda x: stats.skew(x.dropna())).sort_values(ascending=False)
# skewed_feats = train[num_fe].apply(lambda x: stats.skew(x.dropna())).sort_values(ascending=False)
print("\nSkew in numerical features: \n")
skewness = pd.DataFrame({'Skew' :skewed_feats})
skewness.head(10)

skewness = skewness[abs(skewness) > 0.75]
print("There are {} skewed numerical features to Box Cox transform".format(skewness.shape[0]))
skewed_features = skewness.index
lam = 0.15
for feat in skewed_features:
    #all_data[feat] += 1
    Data[feat] = boxcox1p(Data[feat], lam)
# for feat in skewed_features:
#     #all_data[feat] += 1
#     train[feat] = boxcox1p(train[feat], lam)

"""#### Non Ordinal"""

# NonOrdinalVar = Data.select_dtypes(include = ['object']).columns.tolist()
# Data = pd.get_dummies(Data, columns=NonOrdinalVar)
# Encode remaining categorical variables that have not been encoded
categorical_features = Data.select_dtypes(include=['object']).columns.tolist()

# Apply one-hot encoding to all remaining categorical variables
Data = pd.get_dummies(Data, columns=categorical_features)

# Check if there are still any non-numeric columns
print("Remaining non-numeric columns:", Data.select_dtypes(include=['object']).columns.tolist())

"""### Feature Selection"""

def correlation(df, threshold=None):
    # Set of all names of correlated columns
    col_corr = set()
    corr_mat = df.corr()
    for i in range(len(corr_mat.columns)):
        for j in range(i):
            if (abs(corr_mat.iloc[i,j]) > threshold):
                colname = corr_mat.columns[i]
                col_corr.add(colname)
    return col_corr

def reduce_data(df,comp_number=20):
    pca = PCA(n_components=comp_number, random_state=15).fit(df)
    reduced_data = pca.transform(df)
    reduced_data = pd.DataFrame(reduced_data)
    return reduced_data


print(train.columns.tolist())

# Redefine train and test datasets after encoding
train = Data[:ntrain]
test = Data[ntrain:]

# Split data
X_train, X_val, y_train, y_val = train_test_split(train, Target.values, test_size=0.25, random_state=42)

# Train LGBM model
lgbm = LGBMRegressor(random_state=11)
lgbm.fit(X_train, y_train)
pred = lgbm.predict(X_val)
print('The RMSE {}'.format(np.sqrt(mean_squared_error(y_val, pred))))

# """### Split data"""

# X_train, X_val, y_train, y_val = train_test_split(train, Target.values, test_size = 0.25, random_state=42)

# print('The shape of X_train', X_train.shape)
# print('The shape of y_train', y_train.shape)
# print('The shape of X_val', X_val.shape)
# print('The shape of y_val', y_val.shape)

# """### Scaling"""

# # scale = StandardScaler().fit(X_train)
# # X_train = scale.transform(X_train)

# """### Test Models

# #### Linear Regression
# """

# """#### LGBM"""

# lgbm = LGBMRegressor(random_state=11)
# lgbm.fit(X_train, y_train)
# pred = lgbm.predict(X_val)
# print('The RMSE {}'.format(np.sqrt(mean_squared_error(y_val, pred))))


import pickle

# Save the trained model (LGBMRegressor, not predictions)
with open("pred.pkl", "wb") as f:
    pickle.dump(lgbm, f)  # Save the actual model




